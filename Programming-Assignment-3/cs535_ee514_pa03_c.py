# -*- coding: utf-8 -*-
"""CS535_EE514_PA03_C.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iCeg4Fd9coA52ZgSD_fmiZAXSDngMBkt

## CS535/EE514 - Spring 2023 - Assignment 3 - Part C - Naive Bayes

## Marks: 30
## Due Date: April 3, 2025, 23:55

## Instructions

*   Submit your code both as notebook file (.ipynb) and python script (.py) on LMS. The name of both files should be 'RollNo_PA03_C'.

*   The code MUST be implemented independently. Any plagiarism or cheating of work from others or the internet will be immediately referred to the DC.

* 10% penalty per day for 3 days after due date. No submissions will be accepted after  that.

* Use procedural programming style and comment your code properly.

* **Deadline to submit this assignment is April 3, 2025, 23:55**

* Make sure to run all blocks before submission.

### Goal:

The goal of this part of the assignment is to get you familiar with Naive Bayes and to give hands on experience of basic python tools and libraries which will be used in implementing the algorithm.

### Note:

You are <font color="red">not allowed </font> to use any other libraries than the ones imported below for this part. You have to implement your own Naive Bayes classifer from scratch in the first section. Then you will code it using the pre-made Sci-Kit implementation. You are not allowed to use sci-kit learn in the first part except for metrics.

<b>Double click here to enter your name and roll number:  
Name: Farooq Ahmad

Roll Number: 24030021
</b>

# Setup
Import libraries and mount your drive

Link to the Dataset:
[Click Here](https://drive.google.com/drive/folders/1ykjq4ziWjp8Y6DPUO77QnTivhFOnkjSx?usp=sharing)
"""

# Commented out IPython magic to ensure Python compatibility.
import string
import sklearn
import matplotlib.pyplot as plt
# %matplotlib inline
import pandas as pd
import numpy as np
import math
import os
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import GridSearchCV
import re
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import LabelBinarizer
from scipy.special import softmax
from sklearn.feature_extraction.text import CountVectorizer

# from google.colab import drive
# drive.mount('/content/drive')

"""# Read Dataset
Replace the paths with your paths to read the files
"""

#extract files
tr = pd.read_csv("C:\\Users\\PC\\Downloads\\train.csv")
ts = pd.read_csv('C:\\Users\\PC\\Downloads\\test.csv')
stop = pd.read_table('C:\\Users\\PC\\Downloads\\stop_words.txt',header=None)[0]

#separate tweets from files
tr_tweets = tr["Tweet"]
ts_tweets = ts["Tweet"]
tr_labels = tr["Sentiment"]
ts_labels = ts["Sentiment"]
tr_size = len(tr_tweets)
ts_size = len(ts_tweets)

"""# Data Preprocessing
Clean your data to remove unwanted symbols. </br>
The following methods are helpful: </br>


*   [.casefold()](https://www.w3schools.com/python/ref_string_casefold.asp)
*   [.lstrip()](https://www.w3schools.com/python/ref_string_lstrip.asp)
*   [re.sub()](https://www.w3schools.com/python/python_regex.asp)
*   [.rstrip()](https://www.w3schools.com/python/ref_string_rstrip.asp)
*   [.replace](https://www.w3schools.com/python/ref_string_replace.asp)

*Note: You may use other functions for processing but these should be enough*
"""

#Part 1 - Data Preprocessing
# for indx in range(tr_size): # pre-process training data
    #convert strings to lowercase
    #remove usernames and hyperlinks
    #removing digits and next line symbols
    #remove punctuation and symbols
    # {Code here}
    # for word in stop: #remove stop words
      # {Code here}
# for indx in range(ts_size): # pre-process testing data
    #convert strings to lowercase
    #remove usernames and hyperlinks
    #removing digits and next line symbols
    #remove punctuation and symbols
    # {Code here}
    # for word in stop: #remove stop words
      # {Code here}

# Function to clean a single tweet
def clean_tweet(tweet, stop_words):
    # Convert to lowercase
    tweet = tweet.casefold()

    # Remove usernames (@user)
    tweet = re.sub(r'@\w+', '', tweet)

    # Remove hyperlinks
    tweet = re.sub(r'http\S+|www\S+|https\S+', '', tweet, flags=re.MULTILINE)

    # Remove digits and newline symbols
    tweet = re.sub(r'\d+', '', tweet)
    tweet = tweet.replace('\n', ' ')

    # Remove punctuation
    tweet = tweet.translate(str.maketrans('', '', string.punctuation))

    # Remove stop words
    words = tweet.split()
    filtered_words = [word for word in words if word not in stop_words]

    # Join back to a clean sentence
    cleaned = ' '.join(filtered_words)

    return cleaned

# Clean training tweets
for i in range(tr_size):
    tr_tweets[i] = clean_tweet(tr_tweets[i], stop)

# Clean testing tweets
for i in range(ts_size):
    ts_tweets[i] = clean_tweet(ts_tweets[i], stop)

"""# Bag of Words"""

#Part 2 - Bag Of Words
# Extract vocab from training data set
# vocab_list = []
# for indx in range(tr_size):
  # read tweet
  # tr_tweets[indx] = tr_tweets[indx].split()
  # append words from each tweet to vocab_list
  # make sure words don't get repeated
  # {Code Here}

# Create Bag of Words Matrix of size (number of tweets in training data, size of vocabulary)
# each row is a tweet and each column is a word
# matrix = np.zeros(('''Enter size here'''))

# Populate BoW
# Go through the words in each tweet
# for each word append the count of the matrix at relevant position
# {Code Here}


# Step 1: Extract vocabulary from training tweets
vocab_set = set()  # used a set to avoid duplicates

for indx in range(tr_size):
    tweet_words = tr_tweets[indx]
    for word in tweet_words:
        vocab_set.add(word)

# Convert vocab_set to a sorted list for consistent ordering
vocab_list = sorted(list(vocab_set))
vocab_size = len(vocab_list)

# Create a word-to-index dictionary for easy lookup
word_to_index = {word: idx for idx, word in enumerate(vocab_list)}

# Step 2: Create BoW matrix (training)
matrix = np.zeros((tr_size, vocab_size), dtype=int)

for i in range(tr_size):
    for word in tr_tweets[i]:
        if word in word_to_index:
            idx = word_to_index[word]
            matrix[i][idx] += 1

"""# Implement Naive Bayes


*   Step 01: Create a dictionary 'count' to store the number of times a word occurs in each class (positive, negative, neutral). You can do this by using the matrix. For each tweet use the index to access the actual label from tr_labels. </br>
The dictionary will have the following structure: </br>
$count[word] = [positive, negative, neutral]$ </br>
Example: <br/> $count['to'] = [5,10,12]$


"""

# Classes mapping for simplicity
class_labels = {'positive': 0, 'negative': 1, 'neutral': 2}

# Initialize count dictionary
count = {}

for i in range(tr_size):
    label = tr_labels[i]
    label_index = class_labels[label]

    for word in tr_tweets[i]:
        if word not in count:
            count[word] = [0, 0, 0]
        count[word][label_index] += 1

"""*   Step 02: Find the prior probability of each class </br>
To calculate the prior probability of a class calculate the ratio of </br>
$N_{c}$ : $N_{t}$ <br/>
where $N_{c}$ is the number of tweets belonging to the class and $N_{t}$ is the total number of tweets. </br>
Store these in: </br>
$prior = [positivePrior, negativePrior, neutralPrior]$
"""

# Initialize class counters
class_counts = [0, 0, 0]  # [positive, negative, neutral]

# Count how many tweets belong to each class
for label in tr_labels:
    if label == 'positive':
        class_counts[0] += 1
    elif label == 'negative':
        class_counts[1] += 1
    elif label == 'neutral':
        class_counts[2] += 1

# Total number of training tweets
total_tweets = tr_size

# Calculate priors
prior = [count / total_tweets for count in class_counts]

"""*   Step 03: Find the likelihood of each word for every class </br>
Each word will have 3 likelihoods, one corresponding to each class. </br>
Calculate the likelihood of a word specific to a class: <br/>

  *   count($w_{i}$,c) : the number of times the word occurs in the specific class
  *   $|V|$ : the size of vocabulary
  *   Summation count(w,c) : sum of the number of times each word belonging to the vocabulary occurs in the specific class i.e. the word count of this class

> ![image.png](data:image/png;base64,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) <br/>
Store inside a dictionary 'likelihoods' in the form:
$likelihoods[word] = [positiveLikelihood, negativeLikelihood, neutralLikelihood]$ </br>
> For example : </br> $likelihoods['to']=[0.3,0.57,0.23]$
"""

likelihoods = {}

# Likelihoods with Laplace Smoothing
labelsMap = {"positive": 0, "negative": 1, "neutral": 2}

# Step 1: Count total words in each class (denominator of the likelihood formula)
total_word_count_per_class = [0, 0, 0]  # [positive, negative, neutral]

for word, counts in count.items():
    for i in range(3):  # 3 classes
        total_word_count_per_class[i] += counts[i]

# Step 2: Compute likelihoods for each word
vocab_size = len(vocab_list)

for word in vocab_list:
    word_counts = count.get(word, [0, 0, 0])
    word_likelihoods = []

    for i in range(3):  # for each class
        # Apply Laplace smoothing
        likelihood = (word_counts[i] + 1) / (total_word_count_per_class[i] + vocab_size)
        word_likelihoods.append(likelihood)

    likelihoods[word] = word_likelihoods

"""# Predictions and Evaluation
Given a test data point, and the set of prior probabilities and likelihoods, we need to return the 'best' class c. </br>
1. We will create a vector 'prob' of length equal to the number of classes, 3. </br>
$prob = [0,0,0]$ <br/>
2. For each class c, we will initially add our prior probability to prob. </br>
3. Then for each word in the test data in our vocabulary, we will multiply the corresponding likelihood with prob. </br>
4. Finally, the maximum index of our 'prob' vector will be the predicted class for the test data point. </br>
5. Compare these to the actual labels to calculate F1_score, accuracy and confusion matrix. You can use scikit libraries for this. </br>
"""

# Predictions


# Reverse map to get class name from index
indexToLabel = {0: "positive", 1: "negative", 2: "neutral"}

predicted_labels = []

for i, tweet in enumerate(ts_tweets):
    tweet = tweet.split()
    actual_label = labelsMap[ts_labels[i]]

    # Initialize log probabilities for each class
    log_prob = [math.log(prior[c]) for c in range(3)]

    for word in tweet:
        if word in likelihoods:  # ignore words not seen in training
            for c in range(3):
                log_prob[c] += math.log(likelihoods[word][c])

    # Predict the class with the highest log probability
    predicted_class = np.argmax(log_prob)
    predicted_labels.append(indexToLabel[predicted_class])

# Ground truth labels (convert to string to match predicted_labels)
actual_labels = [label for label in ts_labels]

# Evaluation
print("Accuracy:", accuracy_score(actual_labels, predicted_labels))
print("F1 Score (macro):", f1_score(actual_labels, predicted_labels, average='macro'))
print("\nConfusion Matrix:\n", confusion_matrix(actual_labels, predicted_labels))
print("\nClassification Report:\n", classification_report(actual_labels, predicted_labels))

# Evaluation

# Convert actual labels to list (if not already)
actual_labels = list(ts_labels)

# Accuracy
acc = accuracy_score(actual_labels, predicted_labels)
print("Accuracy:", acc)

# F1 Score (macro = treats all classes equally)
f1 = f1_score(actual_labels, predicted_labels, average='macro')
print("F1 Score (macro):", f1)

# Confusion Matrix
conf_matrix = confusion_matrix(actual_labels, predicted_labels)
print("\nConfusion Matrix:\n", conf_matrix)

# Classification Report
print("\nClassification Report:\n", classification_report(actual_labels, predicted_labels))

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=['positive', 'negative', 'neutral'], yticklabels=['positive', 'negative', 'neutral'], cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""# Scikit Implementation of Naive Bayes
Use Scikit-Learn’s implementation of the naive Bayes classifier on the bag of words. Remember to implement one vs rest model with the in-built classifier in binary classification
mode. Report the accuracy, F1 score, and confusion matrix of test using the library’s
implementation.
"""

# {Code Here}


# Convert tweets back into strings for vectorization
train_docs = [" ".join(tweet) if isinstance(tweet, list) else tweet for tweet in tr_tweets]
test_docs = [" ".join(tweet) if isinstance(tweet, list) else tweet for tweet in ts_tweets]

# Vectorize using BoW
vectorizer = CountVectorizer(vocabulary=vocab_list)
X_train = vectorizer.transform(train_docs)
X_test = vectorizer.transform(test_docs)

# Binarize labels for OvR
lb = LabelBinarizer()
Y_train_bin = lb.fit_transform(tr_labels)
class_list = lb.classes_  # ['negative', 'neutral', 'positive']

# Train One-vs-Rest classifiers
models = []
for i in range(3):
    y_binary = Y_train_bin[:, i]  # Binary target for class i vs rest
    model = MultinomialNB()
    model.fit(X_train, y_binary)
    models.append(model)

# Predict probabilities from each classifier
probabilities = np.zeros((X_test.shape[0], 3))

for i in range(3):
    probs = models[i].predict_proba(X_test)[:, 1]  # Get prob for class i
    probabilities[:, i] = probs

# Predict class with highest probability
predicted_class_indices = np.argmax(probabilities, axis=1)
predicted_labels_sklearn = [class_list[i] for i in predicted_class_indices]

# Evaluate
print("Scikit Accuracy:", accuracy_score(ts_labels, predicted_labels_sklearn))
print("Scikit F1 Score (macro):", f1_score(ts_labels, predicted_labels_sklearn, average='macro'))
print("\nScikit Confusion Matrix:\n", confusion_matrix(ts_labels, predicted_labels_sklearn))
print("\nScikit Classification Report:\n", classification_report(ts_labels, predicted_labels_sklearn))

"""# Custom Inference using Scikit-Learn Naive Bayes

Using the implementation of the Navie Bayes classifier, write a function that takes in a tweet, preprocesses it accordingly for the classifier, and feeds into the classifier to predict the label. Apply softmax to get the confidence of the highest prediction. Run your custom inference function with a few unseen tweets (not from the train-test dataset). Print the tweet, the prediction and its confidence. How does the classifier perform on these unseen examples?
"""

# Build a clean vocab from training data
vocab_set = set()
for tweet in tr_tweets:
    if isinstance(tweet, str):
        words = tweet.split()
    else:
        words = tweet
    vocab_set.update(words)

vocab_list = list(vocab_set)
vectorizer = CountVectorizer(vocabulary=vocab_list)

# Fit the vectorizer on processed training data
X_train_vec = vectorizer.transform([' '.join(tweet) if isinstance(tweet, list) else tweet for tweet in tr_tweets])

# One-vs-Rest MultinomialNB models
class_list = ["positive", "negative", "neutral"]
models = []

for sentiment in class_list:
    y_binary = np.array([1 if label == sentiment else 0 for label in tr_labels])
    model = MultinomialNB()
    model.fit(X_train_vec, y_binary)
    models.append(model)

# Preprocessing function
def preprocess_tweet(tweet, stop_words):
    tweet = tweet.casefold()
    tweet = re.sub(r'@\w+', '', tweet)
    tweet = re.sub(r'http\S+', '', tweet)
    tweet = re.sub(r'\d+', '', tweet)
    tweet = tweet.translate(str.maketrans('', '', string.punctuation))
    tweet = tweet.strip()
    words = tweet.split()
    words = [word for word in words if word not in stop_words]
    return " ".join(words)

# Inference function
def custom_infer(tweet, stop_words, models, vectorizer, class_list):
    processed = preprocess_tweet(tweet, stop_words)
    if processed.strip() == "":
        return "unknown", 0.0
    vectorized = vectorizer.transform([processed])
    probs = np.array([model.predict_proba(vectorized)[0][1] for model in models])
    softmax_probs = softmax(probs)
    predicted_index = np.argmax(softmax_probs)
    return class_list[predicted_index], softmax_probs[predicted_index]

# Test unseen tweets
unseen_tweets = [
    "I'm feeling fantastic about the results!",
    "This is the worst product I've ever used.",
    "It's okay I guess. Nothing too exciting.",
    "Ugh, why does this always happen to me?",
    "Absolutely loved the new episode!"
]

print("=== Custom Inference ===\n")
for tweet in unseen_tweets:
    label, confidence = custom_infer(tweet, stop, models, vectorizer, class_list)
    print(f"Tweet: {tweet}")
    print(f"Predicted Label: {label}")
    print(f"Confidence: {confidence:.4f}\n")

"""<font color="green">Answer Here </font> <br>
The classifier performs reasonably well on unseen examples, correctly identifying clear positive and negative sentiments with moderate confidence. It struggles slightly with ambiguous language, reflecting lower confidence in such cases. Overall, it generalizes well given the simplicity of the Custom Inference using Scikit-Learn Naive Bayes model.
"""